{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spazaldinho/DistributiveOverwatch/blob/main/CampQmindxOverwatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Welcome to Overwatch"
      ],
      "metadata": {
        "id": "bgFcHrGQTlSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please save a copy of this Colab so you can make edits.**"
      ],
      "metadata": {
        "id": "sW-j4huZKH7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0) Prepare for AI on Overwatch\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WUTNbqYCPIh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to install all the dependencies we need and set the correct environment variables. Before you start this demo make sure you sign up for an Overwatch account. [Follow this link](https://app.overwatch.distributive.network/). You will also have to sign up for DCP account. The website will prompt you to do this."
      ],
      "metadata": {
        "id": "eSqqlLaJYl0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0.1 Install The Required Dependencies"
      ],
      "metadata": {
        "id": "qKgrELb7U8Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to install overwatch-client so that we run inferences using overwatch. We also need onnx and onnxruntime. This is Microsoft's machine learning library which Overwatch uses."
      ],
      "metadata": {
        "id": "7XMzKcjf7vMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install overwatch-client --upgrade\n",
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "OyRw6RqLPOtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb81f53-b16b-4f12-b88a-28a627b677ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting overwatch-client\n",
            "  Downloading overwatch_client-0.1.7-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: overwatch-client\n",
            "Successfully installed overwatch-client-0.1.7\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.15.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0.2 Import The Necessary Packages"
      ],
      "metadata": {
        "id": "0reC0fJ-VG0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the packages we will need to use for the example below."
      ],
      "metadata": {
        "id": "LzIEosJA7mSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from overwatch_client import HTTPClient\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import files\n",
        "import math\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import shutil\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import sys"
      ],
      "metadata": {
        "id": "5KyEVkb8VWEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0.3 Open The Client\n",
        "This client is what our script will use to communicate with the Overwatch server."
      ],
      "metadata": {
        "id": "GGNsizjVVWl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = HTTPClient()"
      ],
      "metadata": {
        "id": "4AlSD_i0Vr79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0.4 Sign Into The Model Database\n",
        "We need to authenticate ourselves in order to use the Overwatch service. If you have not signed up with Overwatch yet, please follow [this](https://app.overwatch.distributive.network/) link to sign up. Make sure you use your credentials that you signed up for Overwatch with."
      ],
      "metadata": {
        "id": "lo0mab4tVsK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    client.signin(email = 'mehedi@distributive.network',\n",
        "                  password = '123456')\n",
        "except Exception as e:\n",
        "    print('Error: ' + str(e))"
      ],
      "metadata": {
        "id": "VkR2_8oZWfBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0.5 Set Compute Group\n",
        "When we request an inference to be performed, we need to assign it a compute group. A compute group is just a group of computers that work on the same group of tasks. Here we will generate the credentials to work on the ovwatch compute group. If we didn't supply credentials the inference would run on Distributive's Global compute group."
      ],
      "metadata": {
        "id": "vXRsTTcEkRgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joinKey     = \"ovwatch\"  # String\n",
        "joinSecret  = \"0UFRCfojif\"  # String"
      ],
      "metadata": {
        "id": "2drevHC9lpzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0.6 Download Our Model Zoo And Processing Code\n",
        "This repository contains the models, processing code and plotting code for this demo. This command will download it for you and we will make use of it later."
      ],
      "metadata": {
        "id": "nPr3YC9xleZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Distributive-Network/Overwatch-Model-Zoo.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mlu3qAhd7ld",
        "outputId": "b3352410-8ca0-42f7-a43b-d878c92f3bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Overwatch-Model-Zoo'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 104 (delta 39), reused 44 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (104/104), 26.77 KiB | 6.69 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "Downloading FACES/FACES.onnx (1.3 MB)\n",
            "Error downloading object: FACES/FACES.onnx (34cd7e6): Smudge error: Error downloading FACES/FACES.onnx (34cd7e60aeff28744c657de7a3dc64e872d506741de66987f3426f2b79f88017): batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
            "\n",
            "Errors logged to /content/Overwatch-Model-Zoo/.git/lfs/logs/20231105T143031.180381598.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: FACES/FACES.onnx: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Beginner Example"
      ],
      "metadata": {
        "id": "p4ok7MQsTqJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 Model Selection\n",
        "In this example, we will run an inference using Overwatch. To start off select a model that you would like to inference with. We have a list of offical models on our [GitHub](https://github.com/Distributive-Network/Overwatch-Model-Zoo). Pick a model you are interested in and copy its name from the directory. You can broswe through the models by checking out their READMEs.\n",
        "\n",
        "Alternatively, you can do this by running the cell below. It will open up the Overwatch application in this notebook. You can log in using your credentials. Click on the models tab and you will see a list of models to choose from, along with their descriptions. Pick a model and copy its name."
      ],
      "metadata": {
        "id": "Xj5HBws182Lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "website_url = 'https://app.overwatch.distributive.network'\n",
        "\n",
        "IFrame(website_url, width=1200, height=800)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "st2_rFLsPcq4",
        "outputId": "543666f5-57dd-43d4-b734-1d9fefe6670e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7aa1401325f0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1200\"\n",
              "            height=\"800\"\n",
              "            src=\"https://app.overwatch.distributive.network\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2 Set model to inference with\n",
        "Now that you have a model name copied we can procede. Now we need to tell the Overwatch Client to inference with the model that you chose."
      ],
      "metadata": {
        "id": "YwGVSJtfax3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelName   = \"MNIST\"       # Replace your model name here"
      ],
      "metadata": {
        "id": "swotjl5XkIgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3 Choose Files To Inference With\n",
        "Choose which files you would like to inference with. They should be either pngs or jpgs. You can also set the image per slice, the number of images inferenced on per slice. If you set the image per slice to 1, that would mean 1 image would be inferenced per slice sent to each computer. If you don't have any images on your computer feel free to download some images from the web."
      ],
      "metadata": {
        "id": "ywpRNK91msT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded    = files.upload()\n",
        "\n",
        "imgPerSlice = 1            # Edit images per slice here\n",
        "files   = {}\n",
        "\n",
        "for upload_name, upload_data in uploaded.items():\n",
        "  files[upload_name] = upload_data\n",
        "numSlices = math.ceil(len(files)/imgPerSlice)\n",
        "batchSize = math.floor(len(files)/numSlices)\n",
        "\n",
        "print(f\"Will infer on { len(files) } inputs, with { batchSize } inputs per slice, and a total of { numSlices } slices\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "sngC24ThnnDz",
        "outputId": "fdfa7207-d687-4450-c408-c422541a943d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1549d785-2780-4100-8534-ea6c28a54a39\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1549d785-2780-4100-8534-ea6c28a54a39\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving mnist.png to mnist.png\n",
            "Will infer on 1 inputs, with 1 inputs per slice, and a total of 1 slices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.4 Make The Inference Request\n",
        "Here we will send the inference request through the Overwatch Client. No need to change anything."
      ],
      "metadata": {
        "id": "MtJSQJOKo2d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = client.infer(\n",
        "    inputs             = files,\n",
        "    model_name         = modelName,\n",
        "    slice_batch        = batchSize,\n",
        "    inference_id       = \"detecting-faces...\",\n",
        "    compute_group_info = f\"{joinKey}/{joinSecret}\"\n",
        "  )\n",
        "\n",
        "print(\"Done Inferencing!\")"
      ],
      "metadata": {
        "id": "kxlNyVKSo_w0",
        "outputId": "4c891dd0-6d24-48df-dd36-3dad0efc96c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-169be9683955>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m resp = client.infer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minputs\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mslice_batch\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minference_id\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;34m\"detecting-faces...\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/overwatch_client/client.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, inputs, model_name, slice_batch, inference_id, compute_group_info)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         response = self.request_session.post(\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will print the raw results of the inference. The results still need to be applied to their images so if the result don't make sense, don't worry!"
      ],
      "metadata": {
        "id": "qWFcj3d5pHfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp"
      ],
      "metadata": {
        "id": "TjVBRDcOpG_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.5 Plot The Results\n",
        "Now that we have our raw results we can clean them up and plot them nicely so we can better understand the result of the inference we performed. Luckily we have some pre-written plotting code for you to use. In the cell below, we have written a script to import and implement the plotting script for an MNIST inference. If you are using a different model, you can change the script to use your model's plotting script. Anywhere in the cell below it says MNIST, change it to your model's name."
      ],
      "metadata": {
        "id": "ahoJRBh3lStO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('./Overwatch-Model-Zoo')\n",
        "from MNIST.plot import MNIST_plot"
      ],
      "metadata": {
        "id": "M0vPj61ApKZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the plotting, the arguments will not change regardless of what model you are using."
      ],
      "metadata": {
        "id": "JC4wtNE3pbFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_plot(files, resp)"
      ],
      "metadata": {
        "id": "6Ytd6H3PlJOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Intermediate Example"
      ],
      "metadata": {
        "id": "R3U0TdKZTwR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1 Pick A Model To Upload\n",
        "In this example, we are going to learn how to upload your own model to the registry. To start off look through all the models in our model zoo and choose one you are interested in. The link to the github is right here:\n",
        "\n",
        "[Overwatch Model Zoo](https://github.com/Distributive-Network/Overwatch-Model-Zoo)\n",
        "\n",
        "Pick a model you are interested in and copy its name from the GitHub."
      ],
      "metadata": {
        "id": "WgFwtKYTt1bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_selected = 'MNIST.onnx' # This should be the file name of the model. So if the model is called MNIST.onnx on our github it will also be named that here.\n",
        "modelName      = 'MNIST_demo' # This is the name you want the model to have, so this can be anything. It will only be seen by that name on our model registry on our website."
      ],
      "metadata": {
        "id": "hq4pxfbuhFSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 Upload The Model\n",
        "This step will upload the model to Overwatch's model registry."
      ],
      "metadata": {
        "id": "PrQurfXhahT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_prefix = 'Overwatch-Model-Zoo/' + model_selected.split('.')[0] + '/'\n",
        "\n",
        "resp = client.register_model(\n",
        "  model_name \t\t    = modelName,\n",
        "  model_path \t\t    = path_prefix + model_selected,\n",
        "  preprocess_path\t  = path_prefix + 'preprocess.py',\n",
        "  postprocess_path\t= path_prefix + 'postprocess.py',\n",
        "  language \t\t      = 'python',\n",
        "  packages\t\t      = ['numpy','opencv-python']\n",
        ")\n"
      ],
      "metadata": {
        "id": "9xUSns2YPdLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3 Choose Files To Inference With\n",
        "Choose which files you would like to inference with. They should be either pngs or jpgs. You can also set the image per slice, the number of images inferenced on per slice. If you set the image per slice to 1, that would mean 1 image would be inferenced per slice sent to each computer."
      ],
      "metadata": {
        "id": "GJqZFBNZqEXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded    = files.upload()\n",
        "\n",
        "imgPerSlice = 1            # Edit images per slice here\n",
        "files   = {}\n",
        "\n",
        "for upload_name, upload_data in uploaded.items():\n",
        "  files[upload_name] = upload_data\n",
        "numSlices = math.ceil(len(files)/imgPerSlice)\n",
        "batchSize = math.floor(len(files)/numSlices)\n",
        "\n",
        "print(f\"Will infer on { len(files) } inputs, with { batchSize } inputs per slice, and a total of { numSlices } slices\")"
      ],
      "metadata": {
        "id": "Kr3SEu98qEqm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "eade3667-bf4e-44a6-8276-762c98ddbb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0eca8cde-d971-4513-b5c0-8ad069a19e6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0eca8cde-d971-4513-b5c0-8ad069a19e6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving person.jpg to person (3).jpg\n",
            "Saving puppy.jpg to puppy (3).jpg\n",
            "Saving puppy2.jpg to puppy2 (3).jpg\n",
            "Will infer on 3 inputs, with 1 inputs per slice, and a total of 3 slices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.4 Make The Inference Request\n",
        "Here we will send the inference request through the Overwatch Client."
      ],
      "metadata": {
        "id": "FJlkLBJLqQ4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = client.infer(\n",
        "    inputs        = files,\n",
        "    model_name    = modelName,\n",
        "    slice_batch   = batchSize,\n",
        "    inference_id  = \"detecting-faces...\",\n",
        "    compute_group_info = f\"{joinKey}/{joinSecret}\"\n",
        "  )\n",
        "\n",
        "print(\"Done Inferencing!\")"
      ],
      "metadata": {
        "id": "B2JmwQ0hqRHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e7708c-74c2-45ec-f353-0c60446e9076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Inferencing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp)"
      ],
      "metadata": {
        "id": "vaWEUmKqqbvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee7c615-da06-46cb-92c2-02f08b5b0aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Image.jpeg': {'output': [[0.0034078722819685936, 0.02747950702905655, 0.03438807651400566, 0.6144407987594604, 0.00162890728097409, 0.15403009951114655, 0.03564215824007988, 0.023539526388049126, 0.026166079565882683, 0.07927698642015457]]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.5 Plot The Results\n",
        "Now that we have our raw results we can them up and plot them nicely so we can better understand the result of the inference we performed. Luckily we have some pre-written plotting code for you to use with plotting. In the cell below, we have written a script to import and implement the plotting script for an MNIST inference. If you are using a different model, you can change the script to use your model's plotting script. Anywhere in the cell below it says MNIST, change it to your model's name."
      ],
      "metadata": {
        "id": "MkIpLwSCqD4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('./Overwatch-Model-Zoo')\n",
        "from MNIST.plot import MNIST_plot"
      ],
      "metadata": {
        "id": "q3rahlUlp9-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the plotting, the arguments will not change regardless of what model you are using."
      ],
      "metadata": {
        "id": "bWLlTf6kqMuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_plot(files, resp)"
      ],
      "metadata": {
        "id": "dCHDeyALqMC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Advanced Example"
      ],
      "metadata": {
        "id": "nZogD_qcTzM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 Model Selection\n",
        "In this example, we are going to learn how to write our own pre and post processing files. In case you have never done pre and post processing before, we will walk you through the steps for the MNIST model. If you are familiar with the process feel free to select any model and write your own processing scripts.\n",
        "\n",
        "To start off look through all the models in our model zoo and choose one you are interested in. The link to the github is right here:\n",
        "\n",
        "[Overwatch Model Zoo](https://github.com/Distributive-Network/Overwatch-Model-Zoo)\n",
        "\n",
        "Pick a model you are interested in and copy its name from the directory."
      ],
      "metadata": {
        "id": "7lMJN6-layZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2 Download The Model\n",
        "Make sure that you navigate through the colab file structure and download the .onnx file you are interested in. You can upload the model to Netron from your local machine and it will show you the structure of the model. This will help you to understand the structure of the model and how you can write a pre and post processing script."
      ],
      "metadata": {
        "id": "R-xP8_q5rXcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3 Look At Model Input And Output\n",
        "Taking the model .onnx file you just downloaded, you can upload that to Netron in the cell below. This will let you visualize all the model layers. The layers we need to focus on are the input and output layers. They will be the first and second layers respectively. Looking at the tensor property of these layers we can see what shape they want. For instance if the property reads:\n",
        "\n",
        "```\n",
        "tensor: float32[1,1,28,28]\n",
        "```\n",
        "\n",
        "This means that the input will take a tensor that is of shape 1x1x28x28."
      ],
      "metadata": {
        "id": "1D8Wli4Ma7Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "website_url = 'https://netron.app'\n",
        "\n",
        "IFrame(website_url, width=1200, height=800)"
      ],
      "metadata": {
        "id": "_fVFpVBgPdq-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "03dec12c-6d0f-49bc-d7f0-0dc5c8430425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7d6c508c8610>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1200\"\n",
              "            height=\"800\"\n",
              "            src=\"https://netron.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4 Make Pre and Post Processing Files\n",
        "Now that we understand the structure of the inputs and outputs of our model, we are going to make our pre and postprocessing files. The commands below make the files but it is up to us to fill them out."
      ],
      "metadata": {
        "id": "-nCe0zNir0mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch preprocess.py\n",
        "!touch postprocess.py"
      ],
      "metadata": {
        "id": "7fxxvi9lr51Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.5 Write preprocess.py\n",
        "We will walk through writing our preprocess.py file together for an MNIST model. You can open up the preprocess.py file by opening up your file directory tab in Colab. It should be the last icon on the top left hand side of the screen.\n",
        "\n",
        "Now that the file is open we can start writing. First things first we need to import the necessary libraries. In our case we will be using opencv and numpy.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "\n",
        "```\n",
        "We can now create our preprocessing function, it must be called preprocessing as shown below. It also must have the arguments `bytes` and `inputNames`. `bytes` contains the data that will be inferenced upon. It is stored in a buffer. In the case of MNIST, the data is images of handwritten digits. `inputNames` has the list of names that the ONNX model is expecting for the data it recieves. You can think of it as the key to open the ONNX model. In this case, there is only one input to the model so we only pay attention to the first element in that list.\n",
        "\n",
        "```\n",
        "def preprocess(bytes, inputNames):\n",
        "```\n",
        "\n",
        "Now we need to create the object that will be fed to the ONNX model for inferencing. ONNX models take a dictionary so we can create the `feeds` object. We also will need the name of the input to send to the ONNX model. This can be found from the `inputNames` variable as shown below.\n",
        "\n",
        "\n",
        "```\n",
        "  feeds = dict()\n",
        "  inputNames = str(inputNames[0])\n",
        "```\n",
        "\n",
        "Because the data is stored in a buffer in the `bytes` variable we can load the data from the buffer with `np.frombuffer`. We can then decode that into a colour image using opencv's `cv.imdecode` function.\n",
        "\n",
        "```\n",
        "  bytesInput = np.frombuffer( bytes, dtype=np.uint8)\n",
        "  image = cv.imdecode(bytesInput, cv.IMREAD_COLOR)\n",
        "```\n",
        "\n",
        "The MNIST model takes in an image of size **1x1x28x28**.  The reason for this is that each of the images have a size of **28x28**. This means that each side of an MNIST picture has 28 pixels. The MNIST images are also grayscale, this means that the typical 3 colour channels of red, green and blue get condensed to a single grayscale channel. The model also only expects one image at a time so thus the model input tensor dimensions are **1x1x28x28**. Code to convert an arbitrarily sized colour image to a grayscale image is below. The pixel values are also divided by 255 because typical pixel values are between 0-255 but the model expects values between 0 and 1.\n",
        "\n",
        "```\n",
        "  image   = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "  image = image / 255\n",
        "  image = cv.resize(image, (28,28))\n",
        "  image = np.reshape(np.asarray(image.astype(np.float32)), (1, 1, 28, 28))\n",
        "```\n",
        "\n",
        "Now we just need to return the `feeds` dict. The dict key should be the inputNames and the value should be the image.\n",
        "\n",
        "```\n",
        "  feeds[inputNames] = image\n",
        "  return feeds\n",
        "```\n",
        "\n",
        "Save this file and it will be good to go."
      ],
      "metadata": {
        "id": "LyKmML9asch1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.6 Write postprocess.py\n",
        "We will walk through writing our postprocess.py file together for an MNIST model. First things first we need to import the necessary libraries. In our case we will be using numpy.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "```\n",
        "We can now create our preprocessing function, it must be called postprocessing as shown below. It also must have the arguments `out`, `labels` and `outputNames`. `out` contains the results of the inference and is the what we will be analyzing.\n",
        "```\n",
        "def postprocess(out, labels, outputNames):\n",
        "```\n",
        "\n",
        "Now we need to create the object that will be fed to the ONNX model for inferencing. ONNX models take a dictionary so we can create the `outputs` object.\n",
        "\n",
        "\n",
        "```\n",
        "   outputs = dict()\n",
        "```\n",
        "\n",
        "The MNIST Post processing is relatively simple. In the `out` dict, the output of the model is stored. The output is **1x10** tensor with each of the 10 outputs representing a number betwen 0 and 9. In order to find the probability of each number being in the image being inferenced, the softmax function is applied to the results.\n",
        "```\n",
        "  probabilities = np.exp(out[outputNames[0]]) / np.sum(np.exp(out[outputNames[0]]), axis=1, keepdims=True)\n",
        "```\n",
        "Those results are then returned to the user.\n",
        "\n",
        "```\n",
        "  outputs['output'] = probabilities.tolist()\n",
        "  return outputs\n",
        "```\n",
        "\n",
        "Save this file and it will be ready for upload to Overwatch."
      ],
      "metadata": {
        "id": "TwiaJREKHNTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.7 Upload The Model\n",
        "Like in the previous example we can upload our model to Overwatch's model registry."
      ],
      "metadata": {
        "id": "p_BivdEhshov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_selected = 'MNIST.onnx' # This should be the file name of the model. So if the model is called MNIST.onnx on our github it will also be named that here.\n",
        "modelName      = 'MNIST_demo' # This is the name you want the model to have, so this can be anything. It will only be seen by that name on our model registry on our website."
      ],
      "metadata": {
        "id": "2rv01WVqLETd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_prefix = 'Overwatch-Model-Zoo/' + model_selected.split('.')[0] + '/'\n",
        "\n",
        "resp = client.register_model(\n",
        "  model_name \t\t    = modelName,\n",
        "  model_path \t\t    = path_prefix + model_selected,\n",
        "  preprocess_path\t  = 'preprocess.py',\n",
        "  postprocess_path\t= 'postprocess.py',\n",
        "  language \t\t      = 'python',\n",
        "  packages\t\t      = ['numpy','opencv-python']\n",
        ")\n"
      ],
      "metadata": {
        "id": "hp59EB81LE9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.8 Inference\n",
        "\n",
        "Upload files and inference on them with your new model through overwatch.\n"
      ],
      "metadata": {
        "id": "Sd1TNgemLs3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded    = files.upload()\n",
        "\n",
        "imgPerSlice = 1            # Edit images per slice here\n",
        "files   = {}\n",
        "\n",
        "for upload_name, upload_data in uploaded.items():\n",
        "  files[upload_name] = upload_data\n",
        "numSlices = math.ceil(len(files)/imgPerSlice)\n",
        "batchSize = math.floor(len(files)/numSlices)\n",
        "\n",
        "print(f\"Will infer on { len(files) } inputs, with { batchSize } inputs per slice, and a total of { numSlices } slices\")"
      ],
      "metadata": {
        "id": "SDB7p0_OL1LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the inference"
      ],
      "metadata": {
        "id": "LJ7p5buccXR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = client.infer(\n",
        "    inputs        = files,\n",
        "    model_name    = modelName,\n",
        "    slice_batch   = batchSize,\n",
        "    inference_id  = \"detecting-faces...\",\n",
        "    compute_group_info = f\"{joinKey}/{joinSecret}\"\n",
        "  )\n",
        "\n",
        "print(\"Done Inferencing!\")"
      ],
      "metadata": {
        "id": "cS9BuO1FLYsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.9 Plot The Results\n",
        "Now that we have our raw results, we can clean them up and plot them nicely so we can better understand the result of the inference we performed. Luckily we have some pre-written plotting code for you to use with plotting. In the cell below, we have written a script to import and implement the plotting script for an MNIST inference. If you are using a different model, you can change the script to use your model's plotting script. Anywhere in the cell below it says MNIST, change it to your model's name."
      ],
      "metadata": {
        "id": "QiEiWtTlMBX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('./Overwatch-Model-Zoo')\n",
        "from MNIST.plot import MNIST_plot"
      ],
      "metadata": {
        "id": "piRdXC7AMAl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_plot(files, resp)"
      ],
      "metadata": {
        "id": "JizgwlioMVf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Expert Example"
      ],
      "metadata": {
        "id": "EUx5V9Z4UEpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1 Model Conversion\n",
        "In this example we will go through the basics of learning to convert models to the ONNX format. While ONNX now lets you train models, the PyTorch and Tensorflow frameworks are the most popular. Here are some guides on how to convert from those frameworks into ONNX.\n",
        "1. [PyTorch to ONNX Conversion](https://medium.com/deci-ai/tutorial-converting-a-pytorch-model-to-onnx-format-f1bbce156d2a)\n",
        "2. [Tensorflow to ONNX](https://github.com/onnx/tensorflow-onnx)\n",
        "\n",
        "We left some space below for you to convert your model."
      ],
      "metadata": {
        "id": "Umg2lKgUumPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert your model to ONNX here"
      ],
      "metadata": {
        "id": "51qu1xaCPeNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2 Pre And Post Processing\n",
        "Now that you have converted your model, remember you still have to make the pre and post processing files as shown in the Advanced Example. You can always use the [Netron tool](netron.app) to help you! Make sure these scripts are customized for your new model and augment the data in the way you intend, otherwise you will get the wrong results."
      ],
      "metadata": {
        "id": "9ScvRUk87tZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch preprocess.py\n",
        "!touch postprocess.py"
      ],
      "metadata": {
        "id": "W1pVjMp88K_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.3 Upload And Inference With Your Model\n",
        "Look back the Intermediate and Beginner Examples to remember how to upload and inference with your model using the client. You can also write your own plotting script to the better visualize your results after they are returned by Overwatch."
      ],
      "metadata": {
        "id": "CTIaynAs8oGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload your model here"
      ],
      "metadata": {
        "id": "frzJ1m4j9ErK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference with your model here"
      ],
      "metadata": {
        "id": "ye01z9aa9EzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot your results here"
      ],
      "metadata": {
        "id": "Ct03ay4r9E4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5) Wizard Example"
      ],
      "metadata": {
        "id": "x_2hOPJFuu67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have gotten here that is amazing! In this example you will train your own custom model in whatever framework you are most comfortable in! Then you will convert the model to ONNX (unless you trained it in ONNX), write the pre and processing scripts, upload the model with the client and then inference with the client. This example will bring together everything you have learned in previous examples. Here are the steps broken down:\n",
        "\n",
        "\n",
        "1.   Train your own model in whichever framework you want i.e. Tensorflow, PyTorch, ONNX\n",
        "2.   Convert your model to be in ONNX. You will have learned how to do this in the Expert Example.\n",
        "3.   Write the pre and post processing scripts for your model as shown in the Advanced Example.\n",
        "4.   Upload your model and the accompanying processing scripts as shown in the Intermediate Example.\n",
        "5.   Perform an inference with the newly uploaded model as shown in the Beginner Example.\n",
        "\n",
        "Good luck with this challenge!\n",
        "\n"
      ],
      "metadata": {
        "id": "hirW-Ec2uxDr"
      }
    }
  ]
}